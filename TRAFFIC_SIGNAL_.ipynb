{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GMPvIGmxeJE"
      },
      "outputs": [],
      "source": [
        "#run this code in GPU\n",
        "#https://www.kaggle.com/datasets/ahemateja19bec1025/traffic-sign-dataset-classification (KAGGLE Site from where the data is)\n",
        "#Connecting to Google drive because for inference images we've labelled the data in TEST folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTING FILE FROM KAGGLE\n",
        "#Go to Kaggle profice .... Scroll down ... There's API.....You'll get username and password from there\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"nikki2501\" # username from the json file\n",
        "#os.environ['KAGGLE_KEY'] = \"18a26daea5b258038f4b2a03cdfee09f\" # key from the json file"
      ],
      "metadata": {
        "id": "jTL_NrwPxhkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#downloading Kaggle dataset\n",
        "!kaggle datasets download -d ahemateja19bec1025/traffic-sign-dataset-classification"
      ],
      "metadata": {
        "id": "FJSFfMNLxj39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip training data\n",
        "from zipfile import ZipFile\n",
        "file_name = \"/content/traffic-sign-dataset-classification.zip\"\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo7cIowwxv9a",
        "outputId": "d74b5f32-adfa-422e-94fd-2dc02c0d732a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install split-folders\n",
        "#this library is used to divide data to traintest n val "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P46PMSpByeGI",
        "outputId": "79f77770-47ba-4a7c-a8bb-8c491532d56c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from keras.optimizers import Adam\n",
        "#IMPORTING necessary Libraries\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D,Dropout,GlobalAveragePooling2D\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "# import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.image import imread\n",
        "import pathlib\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# %matplotlib inline"
      ],
      "metadata": {
        "id": "5-_IGvJAyh5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SPLITTING DATAa\n",
        "# import splitfolders\n",
        "# # Split with a ratio.\n",
        "# # To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
        "# splitfolders.ratio(\"/content/traffic_Data/DATA\", output=\"FINAL\",\n",
        "#     # seed=1337, \n",
        "#     ratio=(.8, .2), group_prefix=None, move=False) # default values\n",
        "# #.8 is taking 80% for train and .2 means taking 20% for validation"
      ],
      "metadata": {
        "id": "ggkPXbXXyjQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copying path of drive .... saved the files in Google drive .... manually separated the inf data\n",
        "train_dir ='/content/drive/MyDrive/Traffic_signal_/train'\n",
        "test_dir = '/content/drive/MyDrive/Traffic_signal_/test'\n",
        "inf_dir='/content/drive/MyDrive/Traffic_signal_/infer'"
      ],
      "metadata": {
        "id": "rPav4zhc0Cbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining image size and batch size (batch size is )\n",
        "\n",
        "img_width=331\n",
        "img_height=331\n",
        "batch_size=128"
      ],
      "metadata": {
        "id": "JYTZfX4G00IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ImageDataGenerator is used Generate batches of tensor image data with real-time data augmentation.\n",
        "#then here we are splitting the training img folder and separting 20% forvalidation data\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,validation_split=0.2)\n",
        "                                  #  rotation_range=30,\n",
        "                                  #  zoom_range=0.4,\n",
        "                                  #  horizontal_flip=True)\n",
        "\n",
        "#all the above commented can be when we have to augment the images\n",
        "#flow_from_directory Takes the path to a directory, and generates batches of augmented/normalized data\n",
        "#to understand more about flow_from_directory refer this: https://studymachinelearning.com/keras-imagedatagenerator-with-flow_from_directory/\n",
        "#\n",
        "#class_mode : One of \"categorical\", \"binary\", \"sparse\", \"input\", or None. Default: \"categorical\".\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    subset=\"training\",\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=(img_height, img_width))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ0MGtux0pZn",
        "outputId": "62557810-b654-413c-f567-a9d5bc1b95e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3926 images belonging to 58 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                 batch_size=batch_size,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 subset=\"validation\",\n",
        "                                                 target_size=(img_height, img_width))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCE1LCjB0xdM",
        "outputId": "e33a6dff-1a00-417a-8a9a-fdab615fb00d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 956 images belonging to 58 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_datagen = ImageDataGenerator(rescale = 1./255)#rescale = 1/255\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                  # batch_size=batch_size,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  target_size=(img_height, img_width))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y9XmLoJ03iK",
        "outputId": "93c6ed23-a477-456c-f620-fcedd979c31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 652 images belonging to 58 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inf_datagen = ImageDataGenerator(rescale = 1./255)#rescale = 1/255\n",
        "\n",
        "inf_generator = inf_datagen.flow_from_directory(inf_dir,\n",
        "                                                  # batch_size=batch_size,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  target_size=(img_height, img_width))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8MeGiP9NuAO",
        "outputId": "3dca881e-1346-42b8-f214-a87071fc4835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 515 images belonging to 58 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications.nasnet import NASNetLarge"
      ],
      "metadata": {
        "id": "oA_7BCPj05cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape=(331,331,3)"
      ],
      "metadata": {
        "id": "KeE7-_o907FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8waxq_8sJjei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NasNetLarge**"
      ],
      "metadata": {
        "id": "w5caVLfbJkUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.\n",
        "using transfer learning.... imports model from it's stored location. no need to write each and every layer \n",
        "\n",
        "\n",
        "include_top false means because the fully connected layers at the end can only take fixed size inputs, which has been previously defined by the input shape and all processing in the convolutional layers. Any change to the input shape will change the shape of the input to the fully connected layers, making the weights incompatible (matrix sizes don't match and cannot be applied).\n",
        "\n",
        "\n",
        "Imagenet weights is used for images classification,transfer learning model models use the imagenet weights"
      ],
      "metadata": {
        "id": "EpnTJsjAHQ-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transfer learning used , explained above\n",
        "base = NASNetLarge(input_shape,\n",
        "                        include_top=False,\n",
        "                        weights='imagenet')\n",
        "\n",
        "#base.trainable = False"
      ],
      "metadata": {
        "id": "eHXLyT1x1bWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b84789e-3c49-4b22-a946-fa584410f52a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-large-no-top.h5\n",
            "343610240/343610240 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding layers to our model, pooling it\n",
        "#Then mentioning number of classes we have(58 in this dataset)\n",
        "\n",
        "inputs = keras.Input(shape=(331,331,3))\n",
        "# We make sure that the base_model is running in inference mode here,\n",
        "# by passing `training=False`. This is important for fine-tuning, as you will\n",
        "# learn in a few paragraphs.\n",
        "x = base(inputs, training=False)\n",
        "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "# A Dense classifier with a single unit (binary classification)\n",
        "outputs = keras.layers.Dense(58)(x)\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "j3BE3qHI1c9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling the model \n",
        "#optimizer is used to reduce the losses\n",
        "#different type of metric explained here :\"https://keras.io/api/metrics/\"\n",
        "#Losses : The purpose of loss functions is to compute the quantity that a model should seek to minimize during training.\n",
        "model.compile(optimizer=keras.optimizers.Adam(),\n",
        "              loss=keras.losses.CategoricalCrossentropy(from_logits=True),#Computes the crossentropy loss between the labels and predictions.\n",
        "              metrics=[keras.metrics.CategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "p-k2iRtv1ebG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#give the summary about model...even the manually added layers and classes\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPieGDcgzb2i",
        "outputId": "70713049-781d-4fd0-f0a1-6be88a19f7a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 331, 331, 3)]     0         \n",
            "                                                                 \n",
            " NASNet (Functional)         (None, 11, 11, 4032)      84916818  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 4032)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 58)                233914    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 85,150,732\n",
            "Trainable params: 233,914\n",
            "Non-trainable params: 84,916,818\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#fitting the model on daatset and traing our machine, Train_gen and val_gen gives the accuracy by which it works and epochs means how many number of time it gonna run to model again and again\n",
        "model.fit(train_generator, epochs=30, validation_data=val_generator)"
      ],
      "metadata": {
        "id": "DpRZRgSx1gIn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e760a2d-6dc2-4e18-9342-769278f52f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "31/31 [==============================] - 1154s 36s/step - loss: 2.6281 - categorical_accuracy: 0.3943 - val_loss: 2.0031 - val_categorical_accuracy: 0.5429\n",
            "Epoch 2/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 1.3476 - categorical_accuracy: 0.7308 - val_loss: 1.3625 - val_categorical_accuracy: 0.6956\n",
            "Epoch 3/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.8987 - categorical_accuracy: 0.8362 - val_loss: 1.0437 - val_categorical_accuracy: 0.7646\n",
            "Epoch 4/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.6639 - categorical_accuracy: 0.8884 - val_loss: 0.8526 - val_categorical_accuracy: 0.8222\n",
            "Epoch 5/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.5174 - categorical_accuracy: 0.9233 - val_loss: 0.7030 - val_categorical_accuracy: 0.8734\n",
            "Epoch 6/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.4191 - categorical_accuracy: 0.9452 - val_loss: 0.5961 - val_categorical_accuracy: 0.8964\n",
            "Epoch 7/30\n",
            "31/31 [==============================] - 149s 5s/step - loss: 0.3470 - categorical_accuracy: 0.9626 - val_loss: 0.5118 - val_categorical_accuracy: 0.9132\n",
            "Epoch 8/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.2931 - categorical_accuracy: 0.9687 - val_loss: 0.4516 - val_categorical_accuracy: 0.9331\n",
            "Epoch 9/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.2522 - categorical_accuracy: 0.9733 - val_loss: 0.4062 - val_categorical_accuracy: 0.9414\n",
            "Epoch 10/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.2186 - categorical_accuracy: 0.9832 - val_loss: 0.3604 - val_categorical_accuracy: 0.9498\n",
            "Epoch 11/30\n",
            "31/31 [==============================] - 149s 5s/step - loss: 0.1923 - categorical_accuracy: 0.9837 - val_loss: 0.3219 - val_categorical_accuracy: 0.9582\n",
            "Epoch 12/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.1709 - categorical_accuracy: 0.9870 - val_loss: 0.2918 - val_categorical_accuracy: 0.9550\n",
            "Epoch 13/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.1521 - categorical_accuracy: 0.9896 - val_loss: 0.2671 - val_categorical_accuracy: 0.9634\n",
            "Epoch 14/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.1368 - categorical_accuracy: 0.9924 - val_loss: 0.2470 - val_categorical_accuracy: 0.9686\n",
            "Epoch 15/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.1237 - categorical_accuracy: 0.9939 - val_loss: 0.2312 - val_categorical_accuracy: 0.9676\n",
            "Epoch 16/30\n",
            "31/31 [==============================] - 149s 5s/step - loss: 0.1120 - categorical_accuracy: 0.9959 - val_loss: 0.2147 - val_categorical_accuracy: 0.9697\n",
            "Epoch 17/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.1027 - categorical_accuracy: 0.9967 - val_loss: 0.2003 - val_categorical_accuracy: 0.9749\n",
            "Epoch 18/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.0940 - categorical_accuracy: 0.9962 - val_loss: 0.1925 - val_categorical_accuracy: 0.9770\n",
            "Epoch 19/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.0878 - categorical_accuracy: 0.9972 - val_loss: 0.1798 - val_categorical_accuracy: 0.9812\n",
            "Epoch 20/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.0807 - categorical_accuracy: 0.9975 - val_loss: 0.1672 - val_categorical_accuracy: 0.9833\n",
            "Epoch 21/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.0743 - categorical_accuracy: 0.9982 - val_loss: 0.1596 - val_categorical_accuracy: 0.9801\n",
            "Epoch 22/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.0695 - categorical_accuracy: 0.9982 - val_loss: 0.1531 - val_categorical_accuracy: 0.9812\n",
            "Epoch 23/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.0656 - categorical_accuracy: 0.9982 - val_loss: 0.1447 - val_categorical_accuracy: 0.9833\n",
            "Epoch 24/30\n",
            "31/31 [==============================] - 149s 5s/step - loss: 0.0606 - categorical_accuracy: 0.9992 - val_loss: 0.1388 - val_categorical_accuracy: 0.9833\n",
            "Epoch 25/30\n",
            "31/31 [==============================] - 149s 5s/step - loss: 0.0569 - categorical_accuracy: 0.9995 - val_loss: 0.1339 - val_categorical_accuracy: 0.9822\n",
            "Epoch 26/30\n",
            "31/31 [==============================] - 149s 5s/step - loss: 0.0536 - categorical_accuracy: 0.9990 - val_loss: 0.1298 - val_categorical_accuracy: 0.9822\n",
            "Epoch 27/30\n",
            "31/31 [==============================] - 150s 5s/step - loss: 0.0505 - categorical_accuracy: 0.9992 - val_loss: 0.1232 - val_categorical_accuracy: 0.9843\n",
            "Epoch 28/30\n",
            "31/31 [==============================] - 149s 5s/step - loss: 0.0475 - categorical_accuracy: 0.9995 - val_loss: 0.1196 - val_categorical_accuracy: 0.9843\n",
            "Epoch 29/30\n",
            "31/31 [==============================] - 149s 5s/step - loss: 0.0447 - categorical_accuracy: 0.9995 - val_loss: 0.1152 - val_categorical_accuracy: 0.9843\n",
            "Epoch 30/30\n",
            "31/31 [==============================] - 149s 5s/step - loss: 0.0424 - categorical_accuracy: 0.9995 - val_loss: 0.1119 - val_categorical_accuracy: 0.9833\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f637bf72150>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the accuracy of inference dataset\n",
        "model.evaluate(inf_generator)"
      ],
      "metadata": {
        "id": "ZzACi_641iKc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1533a32a-c3bf-405f-dcbb-f3f542ad05c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17/17 [==============================] - 116s 7s/step - loss: 0.3690 - categorical_accuracy: 0.9107\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3690038323402405, 0.9106796383857727]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the accuracy of test dataset\n",
        "model.evaluate(test_generator)"
      ],
      "metadata": {
        "id": "CVLujphjm1fs",
        "outputId": "6f10e0e5-4180-480f-9c58-610737498bc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 141s 7s/step - loss: 0.1048 - categorical_accuracy: 0.9877\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.104767806828022, 0.987730085849762]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Saving model in .h5 file \n",
        "model.save('nn30_1.h5')"
      ],
      "metadata": {
        "id": "LOHg-sCbHFsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Have the streamlit code too \n",
        "\n",
        "# Streamlit use: Streamlit is an open-source app framework for Machine Learning and Data Science teams. Create beautiful web apps in minutes.\n",
        "\n",
        "#run streamlit in your local host like VSCode or Jupyter"
      ],
      "metadata": {
        "id": "QqCw-_HBLRnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze > requirement.txt "
      ],
      "metadata": {
        "id": "D4YXrmMkOW7J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9WwFMsIOYkS",
        "outputId": "9ceadca3-8638-441a-fa88-3677a6051fbb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pipreqs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rdi3eyuMOgXC",
        "outputId": "8ae167e2-505d-4411-8979-8f4219eca920"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pipreqs\n",
            "  Downloading pipreqs-0.4.11-py2.py3-none-any.whl (32 kB)\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting yarg\n",
            "  Downloading yarg-0.1.9-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from yarg->pipreqs) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->yarg->pipreqs) (1.24.3)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=dd2bf506672ea8d0f76b56d3f340092d442c03836d904c8b3c963ff4aed1e102\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built docopt\n",
            "Installing collected packages: yarg, docopt, pipreqs\n",
            "Successfully installed docopt-0.6.2 pipreqs-0.4.11 yarg-0.1.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PE-uvrQeOzNl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}